{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:06.891202Z",
     "start_time": "2024-03-12T12:03:02.347913Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading/Writing Data\n",
    "import os\n",
    "import h5py as hp\n",
    "import numpy as np\n",
    "import urllib\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:06.906215Z",
     "start_time": "2024-03-12T12:03:06.890953Z"
    }
   },
   "outputs": [],
   "source": [
    "electron = \"Electron.hdf5\"\n",
    "photon  = \"Photon.hdf5\"\n",
    "\n",
    "electron_url = 'https://cernbox.cern.ch/remote.php/dav/public-files/FbXw3V4XNyYB3oA/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
    "photon_url = 'https://cernbox.cern.ch/remote.php/dav/public-files/AtBT8y4MiQYFcgc/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:06.908873Z",
     "start_time": "2024-03-12T12:03:06.899857Z"
    }
   },
   "outputs": [],
   "source": [
    "def download(url, filename):\n",
    "\n",
    "    if filename not in os.listdir():\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "download(electron_url, electron)\n",
    "download(photon_url,   photon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:17.088874Z",
     "start_time": "2024-03-12T12:03:06.908806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(498000, 32, 32, 2) (498000,) 498000\n"
     ]
    }
   ],
   "source": [
    "def same_seed(seed):\n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train_valid_split(length, valid_ratio, seed):\n",
    "    '''Split provided training data into training set and validation set'''\n",
    "    valid_set_size = int(valid_ratio * length)\n",
    "    train_set_size = length - valid_set_size\n",
    "    index = range(length)\n",
    "    train_ind, valid_ind = random_split(index, \\\n",
    "    [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
    "    return np.array(train_ind), np.array(valid_ind)\n",
    "\n",
    "def data_read(pa1, pa2):\n",
    "    f1 = hp.File(pa1)\n",
    "    x1 = np.array(f1['X'])\n",
    "    y1 = np.array(f1['y'])\n",
    "\n",
    "    f2 = hp.File(pa2)\n",
    "    y2 = np.array(f2['y'])\n",
    "    x2 = np.array(f2['X'])\n",
    "    \n",
    "    X = np.concatenate([x1, x2], axis=0)\n",
    "    y = np.concatenate([y1, y2], axis=0, dtype=np.float32)\n",
    "    # X = X\n",
    "    length = X.shape[0]\n",
    "\n",
    "    return X, y, length\n",
    "\n",
    "\n",
    "\n",
    "X, y, length = data_read(electron, photon)\n",
    "print(X.shape, y.shape, length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:17.106594Z",
     "start_time": "2024-03-12T12:03:17.093591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo = torch.Size([3, 32, 32, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12, 32, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "foo = X[:3]\n",
    "foo = torch.Tensor(foo)\n",
    "print(f'foo = {foo.shape}')\n",
    "\n",
    "t = nn.Conv2d(32, 12, 1)\n",
    "t(foo).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:17.171039Z",
     "start_time": "2024-03-12T12:03:17.107762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `EPData` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:17.172722Z",
     "start_time": "2024-03-12T12:03:17.112448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "class EPData(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        if self.y is None:\n",
    "            return torch.Tensor([self.X[ind][:, :, 0], self.X[ind][:, :, 1]])\n",
    "        else:\n",
    "            return torch.Tensor([self.X[ind][:, :, 0], self.X[ind][:, :, 1]]), torch.tensor(self.y[ind])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Model` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:17.173738Z",
     "start_time": "2024-03-12T12:03:17.123082Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, bias=False, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut.add_module('conv', nn.Conv2d(in_channels, out_channels, 3, bias=False, padding=0))\n",
    "            self.shortcut.add_module('bn', nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        y = F.relu(self.bn2(self.conv2(y)), inplace=True)\n",
    "        y = y + self.shortcut(x)\n",
    "        # y = F.relu(y, inplace=True)\n",
    "        return y\n",
    "\n",
    "\n",
    "class EPNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EPNet, self).__init__()\n",
    "        shape = config['input_shape']\n",
    "        input_channel = config['input_channels']\n",
    "        self.stage1 = BasicBlock(input_channel, 32)\n",
    "        self.stage2 = BasicBlock(32, 64)\n",
    "        self.stage3 = BasicBlock(64, 32)\n",
    "        with torch.no_grad():\n",
    "            self.feature = self._forward_test(torch.zeros(shape)).view(-1).shape[0]\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.feature, config['n_classes']),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def _forward_test(self, x):\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        # x = self.stage3(x)\n",
    "        x = F.adaptive_avg_pool2d(x, output_size=1)\n",
    "        # print(\"average pool:\", x.shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_test(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Flatten(1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(2048, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:17.174464Z",
     "start_time": "2024-03-12T12:03:17.126150Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=False)\n",
    "# model = test()\n",
    "# model(foo).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configuration\n",
    "\n",
    "#### `mps` on mac, `cuda` on pc and the `parameter` of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:17.215741Z",
     "start_time": "2024-03-12T12:03:17.129120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 5201314, 'select_all': True, 'valid_ratio': 0.2, 'n_epochs': 300, 'n_classes': 1, 'base_channels': 3, 'input_channels': 2, 'input_shape': (1, 2, 32, 32), 'depth': 4, 'block_type': 'basic', 'batch_size': 2048, 'learning_rate': 0.001, 'early_stop': 400, 'save_path': './models/model.ckpt'}\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "# configration of the model\n",
    "config = {\n",
    "    'seed': 5201314,      # Your seed number, you can pick your lucky number. :)\n",
    "    'select_all': True,   # Whether to use all features.\n",
    "    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n",
    "    'n_epochs': 300,     # Number of epochs.\n",
    "    'n_classes': 1,\n",
    "    'base_channels': 3,\n",
    "    'input_channels': 2,\n",
    "    'input_shape': (1, 2, 32, 32),\n",
    "    'depth': 4,\n",
    "    'block_type': 'basic',\n",
    "    'batch_size': 2048,\n",
    "    'learning_rate': 1e-3,\n",
    "    'early_stop': 400,    # If model has not improved for this many consecutive epochs, stop training.\n",
    "    'save_path': './models/model.ckpt'  # Your model will be saved here.\n",
    "}\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:19.017601Z",
     "start_time": "2024-03-12T12:03:17.173225Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(length)\n",
    "train_ind, valid_ind = train_valid_split(length, config['valid_ratio'], config['seed'])\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = X[train_ind], y[train_ind], X[valid_ind], y[valid_ind]\n",
    "\n",
    "train_dataset, valid_dataset = EPData(x_train, y_train), \\\n",
    "                               EPData(x_valid, y_valid)\n",
    "\n",
    "\n",
    "# Pytorch data loader loads pytorch dataset into batches.\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "\n",
    "# test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:19.051242Z",
     "start_time": "2024-03-12T12:03:19.018755Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "model = EPNet(config)\n",
    "# count = 0\n",
    "# for x, y in train_loader:\n",
    "#     count += 1\n",
    "#     if count <= 50:\n",
    "#         print(x.shape)\n",
    "print(train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-12T11:23:37.780681Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T11:23:37.782433Z",
     "start_time": "2024-03-12T11:23:37.781723Z"
    }
   },
   "outputs": [],
   "source": [
    "# # model = test()\n",
    "# train_loss_record = []\n",
    "# valid_loss_record = []\n",
    "# train_steps = []\n",
    "# valid_steps = []\n",
    "\n",
    "# def train(model, train_loader, valid_loader, config, device):\n",
    "\n",
    "#     model.to(device)\n",
    "\n",
    "#     # optimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.85, weight_decay=0.8)\n",
    "\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-5)\n",
    "#     # ll = nn.MSELoss()\n",
    "#     ll = nn.CrossEntropyLoss()\n",
    "\n",
    "#     if not os.path.isdir('./models'):\n",
    "#         os.mkdir('./models')\n",
    "\n",
    "#     n_epochs, best_loss, step, valid_step = config['n_epochs'], -math.inf, 0, 0\n",
    "\n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(211)\n",
    "#     ax.set_title(\"train\")\n",
    "#     bx = fig.add_subplot(212)\n",
    "#     bx.set_title(\"valid\")\n",
    "#     fig.show()\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         model.train()\n",
    "\n",
    "#         for x, y in train_loader:\n",
    "#             loss_record = []\n",
    "#             x = x.to(device)\n",
    "#             y = y.type(torch.float32)\n",
    "#             y = y.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             predic = model(x)\n",
    "#             loss = ll(predic, y)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             loss_record.append(loss.detach().item())\n",
    "#             # print(loss)\n",
    "\n",
    "#         train_loss_record.append(sum(loss_record)/len(loss_record))\n",
    "#         train_steps.append(epoch)\n",
    "#         ax.plot(train_steps, train_loss_record)\n",
    "#         fig.canvas.draw()\n",
    "        \n",
    "#         for x, y in valid_loader:\n",
    "#             loss_record = []\n",
    "#             x, y = x.to(device), y.type(torch.float32).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 model.eval()\n",
    "#                 pre = model(x)\n",
    "#                 loss = ll(pre, y)\n",
    "#             loss_record.append(loss.detach().item())\n",
    "            \n",
    "#         valid_loss_record.append(sum(loss_record)/len(loss_record)) \n",
    "#         valid_steps.append(epoch)\n",
    "#         bx.plot(valid_steps, valid_loss_record)\n",
    "#         fig.canvas.draw()\n",
    "        \n",
    "            \n",
    "\n",
    "# train(model, train_loader, valid_loader, config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T11:23:37.785080Z",
     "start_time": "2024-03-12T11:23:37.782631Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/195 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [31]\u001B[0m, in \u001B[0;36m<cell line: 73>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     70\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mModel is not improving, so we halt the training session.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     71\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [31]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, Valid_loader, config, device)\u001B[0m\n\u001B[1;32m     28\u001B[0m x, y \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device), y\u001B[38;5;241m.\u001B[39mto(device)   \u001B[38;5;66;03m# Move your data to device.\u001B[39;00m\n\u001B[1;32m     29\u001B[0m pred \u001B[38;5;241m=\u001B[39m model(x)\n\u001B[0;32m---> 30\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# print(loss.dtype)\u001B[39;00m\n\u001B[1;32m     33\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()                     \u001B[38;5;66;03m# Compute gradient(backpropagation).\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py:1163\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1164\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1165\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:2996\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   2994\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2995\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 2996\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# model = test()\n",
    "def train(model, train_loader, Valid_loader, config, device):\n",
    "    # ll = nn.MSELoss()\n",
    "    ll = nn.CrossEntropyLoss()\n",
    "\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.8)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=1e-5)\n",
    "    \n",
    "    writer = SummaryWriter() # Writer of tensoboard.\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models')\n",
    "    \n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set your model to train mode.\n",
    "        model.to(device)\n",
    "        loss_record = []\n",
    "\n",
    "        # tqdm is a package to visualize your training progress.\n",
    "        train_pbar = tqdm(train_loader, position=0, leave=True)\n",
    "\n",
    "        for x, y in train_pbar:\n",
    "            # print(x.dtype, y.dtype)\n",
    "            optimizer.zero_grad()               # Set gradient to zero.\n",
    "            x, y = x.to(device), ytype(torch.LongTensor)   .to(device)   # Move your data to device.\n",
    "            pred = model(x)\n",
    "            loss = ll(pred, y)\n",
    "            # print(loss.dtype)\n",
    "            \n",
    "            loss.backward()                     # Compute gradient(backpropagation).\n",
    "            optimizer.step()                    # Update parameters.\n",
    "            step += 1\n",
    "            loss_record.append(loss.detach().item())\n",
    "\n",
    "            # Display current epoch number and loss on tqdm progress bar.\n",
    "            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "            train_pbar.set_postfix({'loss': loss.detach().item()})\n",
    "\n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "\n",
    "        writer.add_scalar('Loss/train', mean_train_loss, step)\n",
    "\n",
    "        model.eval() # Set your model to evaluation mode.\n",
    "        loss_record = []\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                # print(f'pred.shape = {pred.shape} y.shape = {y.shape}')\n",
    "                loss = ll(pred, y)\n",
    "\n",
    "            loss_record.append(loss.item())\n",
    "\n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "        writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return\n",
    "\n",
    "train(model, train_loader, valid_loader, config, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T11:55:34.284985Z",
     "start_time": "2024-03-12T11:55:34.267475Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:03:27.500713Z",
     "start_time": "2024-03-12T12:03:26.954209Z"
    }
   },
   "outputs": [],
   "source": [
    "# valid\n",
    "mol = torch.load(\"./model.ckpt\")\n",
    "\n",
    "model_valid = EPNet(config)\n",
    "model_valid.load_state_dict(mol)\n",
    "model_valid.to(device)\n",
    "right = 0\n",
    "\n",
    "n = 0\n",
    "\n",
    "for x, y in valid_loader:\n",
    "    # mol.eval()\n",
    "    # mol.to('cpu')\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    n += 1\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predict = model_valid(x)\n",
    "        # torch.argmax(predict[i]) == torch.argmax(y):\n",
    "        #     right += 1\n",
    "        right += torch.sum(predict, dim=1 == y, dim=1)\n",
    "        n += config['batch_size']\n",
    "\n",
    "print(right / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T11:23:37.789906Z",
     "start_time": "2024-03-12T11:23:37.785200Z"
    }
   },
   "outputs": [],
   "source": [
    "# valid\n",
    "mol = torch.load(\"./model.ckpt\")\n",
    "model_valid = EPNet(config)\n",
    "model_valid.load_state_dict(mol)\n",
    "right = 0\n",
    "\n",
    "n = 0\n",
    "\n",
    "for x, y in valid_loader:\n",
    "    mol.valid()\n",
    "    mol.to('cpu')\n",
    "    n += 1\n",
    "    with torch.no_grad():\n",
    "        predict = mol(x)\n",
    "        predict = torch.argmax(predict)\n",
    "        right += torch.sum(predict, dim=1 == y, dim=1)\n",
    "        n += config['batch_size']\n",
    "print(right / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
